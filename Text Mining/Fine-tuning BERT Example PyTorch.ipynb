{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine tuning BERT Example PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iPlmLK3TU93",
        "outputId": "1237a298-e675-45f0-8b69-c26d9412836c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy import datasets\n",
        "from torchtext.legacy.data import Field, LabelField, BucketIterator\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "import transformers\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, BertConfig\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed_all(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = Field(tokenize = 'spacy', lower = True) # Indicamos que queremos el texto tokenizado\n",
        "LABEL = LabelField(dtype = torch.int64) # Indicamos que la etiqueta la queremos como un entero\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL) # Descargar el dataset IMDB\n",
        "\n",
        "print(vars(train_data.examples[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUS2Z0m4tFLo",
        "outputId": "7836c417-a487-4de8-e891-2579b2bd50b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ['for', 'those', 'who', 'like', 'their', 'murder', 'mysteries', 'busy', ',', 'this', 'is', 'definitely', 'the', 'one', 'to', 'see', ',', 'as', 'it', 'is', 'chock', 'full', 'of', 'interesting', 'and', 'suspicious', 'characters', ',', 'most', 'of', 'them', 'wealthy', 'long', 'island', 'socialite', 'types', '.', 'as', 'the', 'star', 'detective', ',', 'william', 'powell', 'is', 'alternately', 'starchy', 'and', 'inspired', ',', 'behaving', 'at', 'times', 'as', 'if', 'he', 'and', 'his', 'suit', 'went', 'to', 'the', 'cleaners', 'and', 'got', 'pressed', 'together', '.', 'mary', 'astor', 'is', 'very', 'lovely', 'here', '.', '<', 'br', '/><br', '/>powell', 'had', 'made', 'a', 'career', 'out', 'of', 'playing', 'the', 'lead', 'character', ',', 'philo', 'vance', ',', 'in', 'a', 'series', 'of', 'movies', 'made', 'at', 'a', 'couple', 'of', 'studios', 'over', 'several', 'years', '.', 'in', '-', 'between', 'these', 'films', 'he', 'developed', 'into', 'a', 'somewhat', 'offbeat', 'romantic', 'lead', ',', 'at', 'times', 'even', 'essaying', 'gentleman', 'gangster', 'roles', '.', 'already', 'middle', '-', 'aged', ',', 'he', 'was', 'stuck', 'in', 'somewhat', 'of', 'a', 'career', 'rut', 'by', 'the', 'time', 'this', 'one', 'came', 'along', '.', 'as', 'with', 'so', 'many', 'early', 'talkie', 'stars', ',', 'it', 'seemed', 'that', 'his', 'time', 'had', 'come', 'and', 'gone', ',', 'that', 'he', 'was', 'fine', 'for', 'early', 'depression', 'prohibition', '-', 'era', 'films', ',', 'but', 'that', 'with', 'changing', 'times', 'he', 'was', 'perhaps', 'too', 'mature', 'and', 'dandyish', 'to', 'endure.<br', '/><br', '/>the', 'kennel', 'murder', 'case', ',', 'directed', 'by', 'the', 'criminally', 'neglected', 'michael', 'curtiz', ',', 'is', 'one', 'of', 'the', 'last', 'of', 'the', '\"', 'old', 'powells', '\"', ',', 'while', 'the', 'next', 'year', 'would', 'herald', 'in', 'the', 'first', 'of', 'the', 'new', 'ones', ',', 'the', 'thin', 'man', ',', 'the', 'success', 'of', 'which', 'would', 'catapult', 'its', 'leading', 'players', 'into', 'the', 'hollywood', 'stratosphere', '.', 'in', 'kennel', 'we', 'can', 'see', 'the', 'movies', 'still', 'in', 'a', 'somewhat', 'stiff', ',', 'ritualized', 'pattern', ',', 'as', 'the', 'camera', 'does', 'not', 'move', 'much', ',', 'with', 'the', 'acting', ',', 'like', 'the', 'presentation', ',', 'tending', 'toward', 'the', 'theatrical', '.', 'there', \"'s\", 'no', 'harm', 'in', 'this', 'approach', ',', 'though', ',', 'which', 'has', 'its', 'charms', '.', 'it', 'gives', 'the', 'movie', 'a', 'baroque', 'quality', '.'], 'label': 'pos'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertimos las listas tokenizadas a listas de strings y de etiquetas\n",
        "def token_to_sentence(data):\n",
        "  list_sentence = []\n",
        "  list_labels = []\n",
        "  for sentence in data:\n",
        "    str = \"\"\n",
        "    label = 0\n",
        "    for token in sentence.text:\n",
        "      str += (token + \" \")\n",
        "    if sentence.label == \"pos\":\n",
        "      label = 1\n",
        "    list_labels.append(label)\n",
        "    list_sentence.append(str)\n",
        "  return list_sentence, list_labels\n",
        "\n",
        "train_sentences, train_labels = token_to_sentence(train_data)\n",
        "test_sentences, test_labels = token_to_sentence(test_data)"
      ],
      "metadata": {
        "id": "Xqgi59cvvcUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la versión de BERT a utilizar\n",
        "# Referencia (API): https://huggingface.co/distilbert-base-uncased, paper: https://arxiv.org/abs/1910.01108\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased' # Para utilizar la versión normal de BERT usar: 'bert-base-uncased', 'distilbert-base-uncased'\n",
        "\n",
        "# Tokenizamos las oraciones y les asignamos su respectivo ID de palabra.\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=True) # BertTokenizer, DistilBertTokenizer\n",
        "\n",
        "MAX_LENGTH = 200\n",
        "\n",
        "def BERT_Tokenize(sentences, max_length):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  for sent in sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                       # Instancia a codificar\n",
        "                        add_special_tokens = True,  # Añadir '[CLS]' y '[SEP]'\n",
        "                        max_length = MAX_LENGTH,    # Truncar todas las instancias\n",
        "                        truncation=True,\n",
        "                        padding = 'max_length',   # Añadir padding\n",
        "                        return_attention_mask = True,   # Construir las máscaras de atención.\n",
        "                        return_tensors = 'pt')\n",
        "    \n",
        "    # Añadir la instancia codificada a la lista\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # Añadimos la máscara de atención\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "  return input_ids, attention_masks\n",
        "\n",
        "# Convertir las listas a tensores (conjunto de entrenamiento)\n",
        "input_ids_train, attention_masks_train = BERT_Tokenize(train_sentences, MAX_LENGTH)\n",
        "input_ids_train = torch.cat(input_ids_train, dim=0)\n",
        "attention_masks_train = torch.cat(attention_masks_train, dim=0)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "\n",
        "# Convertir las listas a tensores (conjunto de prueba)\n",
        "input_ids_test, attention_masks_test = BERT_Tokenize(test_sentences, MAX_LENGTH)\n",
        "input_ids_test = torch.cat(input_ids_test, dim=0)\n",
        "attention_masks_test = torch.cat(attention_masks_test, dim=0)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', train_sentences[0])\n",
        "print('Token IDs:', input_ids_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOd8WAYS1oQV",
        "outputId": "f8ea3cf6-fd56-48ab-8538-0582cc9c9b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  for those who like their murder mysteries busy , this is definitely the one to see , as it is chock full of interesting and suspicious characters , most of them wealthy long island socialite types . as the star detective , william powell is alternately starchy and inspired , behaving at times as if he and his suit went to the cleaners and got pressed together . mary astor is very lovely here . < br /><br />powell had made a career out of playing the lead character , philo vance , in a series of movies made at a couple of studios over several years . in - between these films he developed into a somewhat offbeat romantic lead , at times even essaying gentleman gangster roles . already middle - aged , he was stuck in somewhat of a career rut by the time this one came along . as with so many early talkie stars , it seemed that his time had come and gone , that he was fine for early depression prohibition - era films , but that with changing times he was perhaps too mature and dandyish to endure.<br /><br />the kennel murder case , directed by the criminally neglected michael curtiz , is one of the last of the \" old powells \" , while the next year would herald in the first of the new ones , the thin man , the success of which would catapult its leading players into the hollywood stratosphere . in kennel we can see the movies still in a somewhat stiff , ritualized pattern , as the camera does not move much , with the acting , like the presentation , tending toward the theatrical . there 's no harm in this approach , though , which has its charms . it gives the movie a baroque quality . \n",
            "Token IDs: tensor([  101,  2005,  2216,  2040,  2066,  2037,  4028, 15572,  5697,  1010,\n",
            "         2023,  2003,  5791,  1996,  2028,  2000,  2156,  1010,  2004,  2009,\n",
            "         2003, 16480,  3600,  2440,  1997,  5875,  1998, 10027,  3494,  1010,\n",
            "         2087,  1997,  2068,  7272,  2146,  2479,  2591,  4221,  4127,  1012,\n",
            "         2004,  1996,  2732,  6317,  1010,  2520,  8997,  2003, 23554,  2732,\n",
            "        11714,  1998,  4427,  1010,  2022,  3270,  6455,  2012,  2335,  2004,\n",
            "         2065,  2002,  1998,  2010,  4848,  2253,  2000,  1996, 20133,  2015,\n",
            "         1998,  2288,  4508,  2362,  1012,  2984, 25159,  2003,  2200,  8403,\n",
            "         2182,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,\n",
            "         8997,  2018,  2081,  1037,  2476,  2041,  1997,  2652,  1996,  2599,\n",
            "         2839,  1010,  6316,  2080, 16672,  1010,  1999,  1037,  2186,  1997,\n",
            "         5691,  2081,  2012,  1037,  3232,  1997,  4835,  2058,  2195,  2086,\n",
            "         1012,  1999,  1011,  2090,  2122,  3152,  2002,  2764,  2046,  1037,\n",
            "         5399,  2125, 19442,  6298,  2599,  1010,  2012,  2335,  2130,  9491,\n",
            "         2075, 10170, 20067,  4395,  1012,  2525,  2690,  1011,  4793,  1010,\n",
            "         2002,  2001,  5881,  1999,  5399,  1997,  1037,  2476, 21766,  2102,\n",
            "         2011,  1996,  2051,  2023,  2028,  2234,  2247,  1012,  2004,  2007,\n",
            "         2061,  2116,  2220,  2831,  2666,  3340,  1010,  2009,  2790,  2008,\n",
            "         2010,  2051,  2018,  2272,  1998,  2908,  1010,  2008,  2002,  2001,\n",
            "         2986,  2005,  2220,  6245, 13574,  1011,  3690,  3152,  1010,   102])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combina las entradas de entrenamiento y prueba a un único tensor\n",
        "train_dataset = TensorDataset(input_ids_train, attention_masks_train, train_labels)\n",
        "test_dataset = TensorDataset(input_ids_test, attention_masks_test, test_labels)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Creamos los DataLoaders para el entrenamiento y prueba\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # Instancias de entrenamiento\n",
        "            sampler = RandomSampler(train_dataset), # Selecciona los lotes de forma aleatoria\n",
        "            batch_size = BATCH_SIZE # Entrena con el tamaño de lote\n",
        "        )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset, # Instancias de prueba\n",
        "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
        "            batch_size = BATCH_SIZE # Evaluate with this batch size.\n",
        "        )"
      ],
      "metadata": {
        "id": "yfwDuDyr8eul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el modelo pre-entrenado BERT-base para la clasificación de una secuencia\n",
        "model = BertForSequenceClassification.from_pretrained( # DistilBertForSequenceClassification, BertForSequenceClassification\n",
        "    PRE_TRAINED_MODEL_NAME, # Cargamos el modelo BASE de BERT\n",
        "    num_labels = 2, # Número de clases\n",
        "    output_attentions = False, # Regresar valores de atención\n",
        "    output_hidden_states = False, # Regresar los valores de codificación\n",
        ")\n",
        "\n",
        "# Asignamos el dispositivo en el que se entrenará el modelo propuesto\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Movemos el modelo a la GPU\n",
        "model.to(device)\n",
        "\n",
        "# Definimos número de épocas\n",
        "epochs = 4\n",
        "\n",
        "# Creamos el optimizador\n",
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "# Definimos el algorítmo de optimización\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Definimos el número total de pasos de entrenamiento (epocas * número de lotes)\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Crear un calendario para actualizar la tasa de aprendizaje (opcional)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5qQIYtSEUNR",
        "outputId": "4b802bc1-458a-4bf2-83ac-0e299bf3c85f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula el total de etiquetas correctamente clasificadas\n",
        "def sum_correct(preds, labels):\n",
        "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  return np.sum(pred_flat == labels_flat), pred_flat, labels_flat\n",
        "\n",
        "# Función para el entrenamiento de nuestro modelo\n",
        "def train(model, iterator, optimizer=optimizer, clip=1):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    total_correct = 0\n",
        "    total_count = 0\n",
        "    \n",
        "    for step, batch in enumerate(iterator):\n",
        "\n",
        "      b_input_ids = batch[0].to(device)\n",
        "      b_input_mask = batch[1].to(device)\n",
        "      b_labels = batch[2].to(device)\n",
        "\n",
        "      # Resetea el valor de los gradientes\n",
        "      model.zero_grad()\n",
        "\n",
        "      outputs  = model(     input_ids = b_input_ids,\n",
        "                            attention_mask = b_input_mask, \n",
        "                            labels = b_labels)\n",
        "      \n",
        "      loss, logits = outputs[:2]\n",
        "      \n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      # Mover las etiquetas y los registros a la cpu\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "      # Sumar las predicciones correctas\n",
        "      correct, predictions, labels = sum_correct(logits, label_ids)\n",
        "      total_correct += correct\n",
        "      total_count += len(b_labels)\n",
        "\n",
        "      # Calcula los gradientes\n",
        "      loss.backward()\n",
        "      # Esto es para ayudar a prevenir el problema de \"explosión de gradientes\".\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "      # Actualiza los pesos del modelo\n",
        "      optimizer.step()\n",
        "      # Actualiza la tasa de aprendizaje\n",
        "      scheduler.step()\n",
        "    \n",
        "    print(f'Train accuracy: {(total_correct/total_count):.6f}')\n",
        "    mean_loss = epoch_loss / len(train_dataloader)\n",
        "    return mean_loss # Pérdida promedio"
      ],
      "metadata": {
        "id": "Rz7Y1SgIF8Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ciclo de entrenamiento\n",
        "for epoch in range(epochs):\n",
        "  result = train(model=model, iterator=train_dataloader)\n",
        "  print(f'Epoch {epoch + 1} / {epochs}, Mean loss: {result:.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BYtTOSFJqyd",
        "outputId": "ab8dcc30-7b12-41d5-cb84-9546b2b00329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 0.873440\n",
            "Epoch 1 / 4, Mean loss: 0.296311\n",
            "Train accuracy: 0.943520\n",
            "Epoch 2 / 4, Mean loss: 0.152798\n",
            "Train accuracy: 0.978520\n",
            "Epoch 3 / 4, Mean loss: 0.069406\n",
            "Train accuracy: 0.991360\n",
            "Epoch 4 / 4, Mean loss: 0.034112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_correct = 0\n",
        "total_count = 0\n",
        "model_prediction = []\n",
        "ground_truth = []\n",
        "model.eval()\n",
        "\n",
        "for batch in test_dataloader:\n",
        "  b_input_ids = batch[0].to(device)\n",
        "  b_input_mask = batch[1].to(device)\n",
        "  b_labels = batch[2].to(device)\n",
        "\n",
        "  with torch.no_grad():        \n",
        "    outputs = model(      input_ids = b_input_ids, \n",
        "                          attention_mask = b_input_mask,\n",
        "                          labels = b_labels)\n",
        "    loss, logits = outputs[:2]\n",
        "\n",
        "  # Mover las etiquetas y los registros a la cpu\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  # Suma las predicciones correctas\n",
        "  correct, predictions, labels = sum_correct(logits, label_ids)\n",
        "  total_correct += correct\n",
        "  total_count += len(b_labels)\n",
        "  model_prediction.append(predictions.tolist())\n",
        "  ground_truth.append(labels.tolist())\n",
        "\n",
        "print(f'Test accuracy: {(total_correct/total_count):.6f}')"
      ],
      "metadata": {
        "id": "9DQJKuFhlxCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2edead3a-4b77-438a-dbbb-32661ec46aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.909400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reporte de clasificación\n",
        "model_prediction = [item for sublist in model_prediction for item in sublist]\n",
        "ground_truth = [item for sublist in ground_truth for item in sublist]\n",
        "print(classification_report(ground_truth, model_prediction, labels=[0 ,1], digits=4))"
      ],
      "metadata": {
        "id": "72oFdxnfhJXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad29ba68-95ef-464a-e125-b4c5164b0888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9242    0.8920    0.9078     12500\n",
            "           1     0.8956    0.9268    0.9109     12500\n",
            "\n",
            "    accuracy                         0.9094     25000\n",
            "   macro avg     0.9099    0.9094    0.9094     25000\n",
            "weighted avg     0.9099    0.9094    0.9094     25000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}