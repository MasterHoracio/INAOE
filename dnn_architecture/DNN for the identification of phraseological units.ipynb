{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import libraries for loading pre-trained word vectors\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#Import libraries for loading and pre-processing the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "#Avoid warnings from libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "\n",
    "#Embeddings\n",
    "path_wordvectors = '../pre-trained embeddings/spanish/'\n",
    "name_wordvectors = 'cc_es_300.vec'\n",
    "type_wordvectors = 'fasttext'\n",
    "wv_dimension     = 300\n",
    "\n",
    "#Dataset\n",
    "path_ds     = '../data/spanish/'\n",
    "name_ds     = 'MEX-A3T/'\n",
    "validation  = False\n",
    "language    = 'sp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset / index: 0 = train, 1 = test, 2 = validation\n",
    "def load_data(path, name, validation = False):#loads the CSV files from datasets\n",
    "    d_train = pd.read_csv(path + name + \"ds_train.csv\")\n",
    "    d_test  = pd.read_csv(path + name + \"ds_test.csv\")\n",
    "    if validation == True:\n",
    "        d_validation = pd.read_csv(path + name + \"_validation.csv\")\n",
    "        return [d_train, d_test, d_validation]\n",
    "    return [d_train, d_test]\n",
    "\n",
    "dataset = load_data(path_ds, name_ds, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  Soy el Clint Eastwood de los Puentes de Madiso...      0\n",
      "1  Actualmente ya pasÃ³ de moda la pucha joto, aho...      0\n",
      "2  Â¿Es cierto esto? Y no me refiero a lo que dijo...      0\n",
      "3  Vuela pega y esquiva... la neta estÃ¡ de la ver...      0\n",
      "4  Mejor puto disfraz de la noche!!!! ðŸ‘ŠðŸ‘ŠðŸ‘ŠPor terc...      0\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process text on dataset\n",
    "def remove_consecutive_characters(sentence):\n",
    "    string_len = len(sentence)\n",
    "    clipped_sentence = ''\n",
    "    queued_character = ''\n",
    "    for index, character in enumerate(sentence):\n",
    "        if index == 0:\n",
    "            clipped_sentence += character\n",
    "            queued_character = character\n",
    "        elif character.isalnum() or character == ' ':\n",
    "            clipped_sentence += character\n",
    "            queued_character = character\n",
    "        else:\n",
    "            if queued_character is not character:\n",
    "                clipped_sentence += character\n",
    "                queued_character = character\n",
    "    return clipped_sentence\n",
    "\n",
    "def clean_text(sentence, language):\n",
    "    #Convert instance to string\n",
    "    sentence = str(sentence)\n",
    "    \n",
    "    #All text to lowecase\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    #Remove all consecutive special characters (non-alphanumeric)\n",
    "    sentence = remove_consecutive_characters(sentence)\n",
    "    \n",
    "    #separate special characters\n",
    "    sentence = re.sub(r\":\", \" : \", sentence)\n",
    "    sentence = re.sub(r\",\", \" , \", sentence)\n",
    "    sentence = re.sub(r\"\\.\", \" . \", sentence)\n",
    "    sentence = re.sub(r\"!\", \" ! \", sentence)\n",
    "    sentence = re.sub(r\"Â¡\", \" Â¡ \", sentence)\n",
    "    sentence = re.sub(r\"â€œ\", \" â€œ \", sentence)\n",
    "    sentence = re.sub(r\"â€\", \" â€ \", sentence)\n",
    "    sentence = re.sub(r\"\\(\", \" ( \", sentence)\n",
    "    sentence = re.sub(r\"\\)\", \" ) \", sentence)\n",
    "    sentence = re.sub(r\"\\?\", \" ? \", sentence)\n",
    "    sentence = re.sub(r\"\\Â¿\", \" Â¿ \", sentence)\n",
    "    \n",
    "    #Split all emojis\n",
    "    emoji_list = emoji.emoji_lis(sentence)\n",
    "    for index, _emoji in enumerate(emoji_list):\n",
    "        location = _emoji['location'] + (index * 2)\n",
    "        sentence = sentence[0:location] + ' ' + sentence[location:location+1] + ' ' + sentence[location+1:]\n",
    "    \n",
    "    #Substituting multiple spaces with single space\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence, flags=re.I)\n",
    "    \n",
    "    tokens = sentence.split()\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "def preprocess_dataset(dataset, language):\n",
    "    for data in dataset:\n",
    "        for i, row in data.iterrows():\n",
    "            if row['text'].strip() != '':\n",
    "                sentence = clean_text(row['text'], language)\n",
    "                data.at[i, 'text'] = sentence\n",
    "    return dataset\n",
    "\n",
    "dataset = preprocess_dataset(dataset, language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  soy el clint eastwood de los puentes de madiso...      0\n",
      "1  actualmente ya pasÃ³ de moda la pucha joto , ah...      0\n",
      "2  Â¿ es cierto esto ? y no me refiero a lo que di...      0\n",
      "3  vuela pega y esquiva . la neta estÃ¡ de la verg...      0\n",
      "4  mejor puto disfraz de la noche ! ðŸ‘Š ðŸ‘Š ðŸ‘Š por ter...      0\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre-trained word embeddings\n",
    "class load_wordvectors():#loads the word vectors\n",
    "    def __init__(self, path, name, type_wv):\n",
    "        self.type_wv = type_wv\n",
    "        self.path    = path\n",
    "        self.name    = name\n",
    "        \n",
    "    def load(self):#returns the loaded word vectors\n",
    "        if self.type_wv == \"custom\":\n",
    "            model = FastText.load(self.path + self.name)\n",
    "        elif self.type_wv == \"fasttext\":\n",
    "            model = KeyedVectors.load_word2vec_format(self.path+self.name, binary=False)\n",
    "        elif self.type_wv == \"glove\":\n",
    "            model = KeyedVectors.load_word2vec_format(self.path+self.name, binary=False)\n",
    "        return model\n",
    "\n",
    "    def tokenize(self, text):#return the tokenizer and the vocabulary size\n",
    "        vocab_set   = list(set().union(text))\n",
    "        tokenizer   = Tokenizer()\n",
    "        tokenizer.fit_on_texts(vocab_set)\n",
    "        vocabulary_sz \t= len(tokenizer.word_index) + 1\n",
    "        return tokenizer, vocabulary_sz\n",
    "\n",
    "    def build_embedding_matrix(self, vocabulary_sz, wv_dimension, tokenizer, model):\n",
    "        embedding_matrix = np.zeros((vocabulary_sz, wv_dimension))\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            if word in model.wv:\n",
    "                embedding_matrix[i] = model.wv[word]\n",
    "        return embedding_matrix\n",
    "\n",
    "wv_model      = load_wordvectors(path_wordvectors, name_wordvectors, type_wordvectors)\n",
    "word_vectors  = wv_model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tokenize the train set\n",
    "tokenizer, vocabulary_sz = wv_model.tokenize(dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the embedding matrix\n",
    "embedding_matrix = wv_model.build_embedding_matrix(vocabulary_sz, wv_dimension, tokenizer, word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dnn variables\n",
    "seq_len            = 40\n",
    "learning_rate      = 0.001\n",
    "word_encoding_dim  = 64\n",
    "n_classes          = 2\n",
    "dropout_rate       = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the text and labels for the neural network\n",
    "class normalize_data():#normalize the text and labels\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def create_label_vector(self, labels, classes):\n",
    "        vector = np.zeros((labels.shape[0], classes))\n",
    "        for instance, label in enumerate(labels):\n",
    "            vector[instance][label] = 1\n",
    "        return vector\n",
    "    def normalize(self, tokenizer, n_classes, seq_len):\n",
    "        n_partition = len(self.dataset)\n",
    "        if n_partition >= 2:\n",
    "            y_train = self.create_label_vector(self.dataset[0]['label'], n_classes)\n",
    "            y_test  = self.create_label_vector(self.dataset[1]['label'], n_classes)\n",
    "            x_train = pad_sequences(tokenizer.texts_to_sequences(self.dataset[0]['text']), maxlen=seq_len)\n",
    "            x_test  = pad_sequences(tokenizer.texts_to_sequences(self.dataset[1]['text']), maxlen=seq_len)\n",
    "        if n_partition == 3:\n",
    "            y_validation = self.create_label_vector(self.dataset[2]['label'], n_classes)\n",
    "            x_validation = pad_sequences(tokenizer.texts_to_sequences(self.dataset[2]['text']), maxlen=seq_len)\n",
    "            return x_train, y_train, x_test, y_test, x_validation, y_validation\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "normalized_data = normalize_data(dataset)\n",
    "x_train, y_train, x_test, y_test = normalized_data.normalize(tokenizer, n_classes, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Embedding, GRU, Bidirectional, concatenate, GlobalMaxPooling1D\n",
    "from keras.layers import Activation, Dense, Conv1D, MaxPooling1D, Dropout, AveragePooling1D, GlobalAveragePooling1D\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras import initializers, layers\n",
    "from keras.optimizers import *\n",
    "from keras import regularizers, initializers, constraints\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from sklearn import metrics\n",
    "\n",
    "#Attention Class\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(name='{}_W'.format(self.name),\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint,\n",
    "                                 trainable=True)\n",
    "        \n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(name='{}_b'.format(self.name),\n",
    "                                     shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint,\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            self.b = None\n",
    "            \n",
    "        self.context = self.add_weight(\n",
    "                                    name='context_vector', shape=(self.step_dim, 1),\n",
    "                                    initializer= self.init,\n",
    "                                    regularizer=self.b_regularizer,\n",
    "                                    constraint=self.b_constraint,\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "        \n",
    "        similarity = K.dot(e, self.context)\n",
    "        a = K.exp(similarity)\n",
    "\n",
    "        #Avoid NaN's with the addition of a very small positive number.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        \n",
    "        #Weighted sum\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n",
    "\n",
    "#create class for the dnn handler\n",
    "class deep_neural_network_handler():\n",
    "    def __init__(self, seq_len, vocabulary_sz, wv_dimension, embedding_matrix, learning_rate, word_encoding_dim, n_classes):\n",
    "        self.seq_len \t\t\t= seq_len\n",
    "        self.vocabulary_sz \t\t= vocabulary_sz\n",
    "        self.wv_dimension \t\t= wv_dimension\n",
    "        self.embedding_matrix \t= embedding_matrix\n",
    "        self.learning_rate \t\t= learning_rate\n",
    "        self.word_encoding_dim  = word_encoding_dim\n",
    "        self.n_classes \t\t\t= n_classes\n",
    "    def get_metrics(self):\n",
    "        if self.n_classes > 2:\n",
    "            custom_loss      = 'categorical_crossentropy'\n",
    "            custom_accuracy  = 'categorical_accuracy'\n",
    "        else:\n",
    "            custom_loss      = 'binary_crossentropy'\n",
    "            custom_accuracy  = 'accuracy'\n",
    "        return custom_loss, custom_accuracy\n",
    "    def model_test(self, model, test_partition):\n",
    "        prediction = model.predict(test_partition)\n",
    "        test_prediction = []\n",
    "        for instance in prediction:\n",
    "            custom_list = instance.tolist()\n",
    "            test_prediction.append(custom_list.index(max(custom_list)))\n",
    "        return test_prediction\n",
    "    def model_test_report(self, model, test_partition, y_test, precision_digits):\n",
    "        test_prediction = self.model_test(model, test_partition)\n",
    "        report          = metrics.classification_report(y_test, test_prediction, digits=precision_digits)\n",
    "        return report\n",
    "    def gru_architecture(self, rnn_dropout_rate=0.15):\n",
    "        instance_input       = Input(shape=(self.seq_len,), dtype='int32')\n",
    "        embedded_sequences   = Embedding(self.vocabulary_sz, self.wv_dimension, weights = [self.embedding_matrix], \n",
    "                                    input_shape=(self.seq_len,), trainable=False)(instance_input)\n",
    "        dropout              = Dropout(rnn_dropout_rate)(embedded_sequences)\n",
    "        gru_uno              = Bidirectional(GRU(units=self.word_encoding_dim, return_sequences=True))(dropout)\n",
    "        gru_dos              = Bidirectional(GRU(units=int(self.word_encoding_dim), return_sequences=False))(gru_uno)\n",
    "        dropout              = Dropout(rnn_dropout_rate)(gru_dos)\n",
    "        dense                = Dense(128, activation='relu')(dropout)\n",
    "        dropout              = Dropout(rnn_dropout_rate)(dense)\n",
    "        prediction           = Dense(self.n_classes, activation='softmax')(dropout)\n",
    "\n",
    "        custom_loss, custom_accuracy = self.get_metrics()\n",
    "\n",
    "        model                = Model(instance_input, prediction)\n",
    "\n",
    "        model.compile(loss        = custom_loss,\n",
    "                        optimizer     = Adam(lr = self.learning_rate),\n",
    "                        metrics       = [custom_accuracy])\n",
    "        return model\n",
    "    def gru_att_architecture(self, rnn_dropout_rate=0.15):\n",
    "        instance_input       = Input(shape=(self.seq_len,), dtype='int32')\n",
    "        embedded_sequences   = Embedding(self.vocabulary_sz, self.wv_dimension, weights = [self.embedding_matrix], \n",
    "                                    input_shape=(self.seq_len,), trainable=False)(instance_input)\n",
    "        dropout              = Dropout(rnn_dropout_rate)(embedded_sequences)\n",
    "        gru_uno              = Bidirectional(GRU(units=self.word_encoding_dim, return_sequences=True))(dropout)\n",
    "        gru_dos              = Bidirectional(GRU(units=int(self.word_encoding_dim), return_sequences=True))(gru_uno)\n",
    "        attention            = Attention(self.seq_len)(gru_dos)\n",
    "        dropout              = Dropout(rnn_dropout_rate)(attention)\n",
    "        dense                = Dense(128, activation='relu')(dropout)\n",
    "        dropout              = Dropout(rnn_dropout_rate)(dense)\n",
    "        prediction           = Dense(self.n_classes, activation='softmax')(dropout)\n",
    "\n",
    "        custom_loss, custom_accuracy = self.get_metrics()\n",
    "\n",
    "        model                = Model(instance_input, prediction)\n",
    "\n",
    "        model.compile(loss        = custom_loss,\n",
    "                        optimizer     = Adam(lr = self.learning_rate),\n",
    "                        metrics       = [custom_accuracy])\n",
    "        return model\n",
    "\n",
    "\n",
    "class validation_metrics(Callback):\n",
    "    def __init__(self):\n",
    "        super(validation_metrics, self).__init__()\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.f1_macro \t\t= []\n",
    "        self.patience \t\t= 5\n",
    "        self.max_f1_macro \t= 0.0\n",
    "        self.negative_count = 0\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(len(self.validation_data) == 6):\n",
    "            val_predict = (np.asarray(self.model.predict([self.validation_data[0],self.validation_data[1],self.validation_data[2]]))).round()\n",
    "            val_targ = self.validation_data[3]\n",
    "        else:\n",
    "            val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "            val_targ = self.validation_data[1]\n",
    "        score = metrics.f1_score(val_targ, val_predict, average='macro')\n",
    "        self.f1_macro.append(score)\n",
    "        if self.max_f1_macro < score:\n",
    "            print(\"THE MACRO AVG F1 SCORE IMPROVED FROM: \" + str(self.max_f1_macro) + \" TO: \" + str(score))\n",
    "            self.max_f1_macro = score\n",
    "            self.negative_count = 0\n",
    "            self.model.save('trained models/current_model.hdf5')\n",
    "        else:\n",
    "            print(\"THE MACRO AVG F1 SCORE DID'T IMPROVE: \" + str(self.max_f1_macro))\n",
    "            self.negative_count += 1\n",
    "        if self.negative_count == self.patience:\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the deep neural network instance\n",
    "dnn_handler = deep_neural_network_handler(seq_len, vocabulary_sz, wv_dimension, embedding_matrix, learning_rate, word_encoding_dim, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create GRU DNN\n",
    "gru_model = dnn_handler.gru_architecture(0.15)\n",
    "gru_model.summary()\n",
    "\n",
    "\n",
    "#Train GRU DNN model\n",
    "c_metrics = validation_metrics()\n",
    "\n",
    "history = gru_model.fit(x                = x_train,\n",
    "                        y                = y_train,\n",
    "                        epochs           = 25,\n",
    "                        batch_size       = 32,\n",
    "                        verbose          = 1,\n",
    "                        validation_split = 0.15,\n",
    "                        callbacks        = [c_metrics])\n",
    "\n",
    "#load trained weights\n",
    "gru_model.load_weights('trained models/current_model.hdf5')\n",
    "\n",
    "#test GRU NN model\n",
    "report = dnn_handler.model_test_report(gru_model, x_test, dataset[1]['label'], 5)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create GRU+ATT DNN\n",
    "gru_model = dnn_handler.gru_att_architecture(0.15)\n",
    "gru_model.summary()\n",
    "\n",
    "\n",
    "#Train GRU DNN model\n",
    "c_metrics = validation_metrics()\n",
    "\n",
    "history = gru_model.fit(x                = x_train,\n",
    "                        y                = y_train,\n",
    "                        epochs           = 25,\n",
    "                        batch_size       = 32,\n",
    "                        verbose          = 1,\n",
    "                        validation_split = 0.10,\n",
    "                        callbacks        = [c_metrics])\n",
    "\n",
    "#load trained weights\n",
    "gru_model.load_weights('trained models/current_model.hdf5')\n",
    "\n",
    "#test GRU NN model\n",
    "report = dnn_handler.model_test_report(gru_model, x_test, dataset[1]['label'], 5)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2_proyects",
   "language": "python",
   "name": "tf_2_proyects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
